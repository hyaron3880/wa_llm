"""On-demand ambient conversation digest.

When the bot is mentioned, this module summarises what the group discussed
recently (up to 4 hours / 100 messages) *outside* of the immediate context
window.  The summary is generated by the cheap/fast model and cached per
group with a 30-minute TTL so repeated mentions don't re-summarise.
"""

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Set

from cachetools import TTLCache
from pydantic_ai import Agent
from sqlmodel import select, desc
from sqlmodel.ext.asyncio.session import AsyncSession

from models import Message
from utils.chat_text import chat2text
from utils.opt_out import get_opt_out_map

logger = logging.getLogger(__name__)

# Cache: (group_jid) -> digest string, 30-minute TTL
# NOTE: cache mutations are intentional — this is stateful storage
_digest_cache: TTLCache[str, str] = TTLCache(maxsize=200, ttl=30 * 60)

# Per-group locks to prevent duplicate LLM calls for the same group
_digest_locks: dict[str, asyncio.Lock] = {}
_locks_lock = asyncio.Lock()

_MAX_DIGEST_MESSAGES = 100
_LOOKBACK_HOURS = 4

_DIGEST_SYSTEM_PROMPT = (
    "You are summarising a WhatsApp group conversation. "
    "Write 2-3 concise sentences in English capturing the main topics discussed. "
    "Do NOT include greetings or filler. Focus on substance."
)


async def get_conversation_digest(
    session: AsyncSession,
    message: Message,
    model_name: str,
    context_ids: Set[str],
    bot_jid: str | None = None,
) -> str:
    """Return a short digest of recent group activity.

    Args:
        session: DB session.
        message: The triggering message (used for group/chat context).
        model_name: Cheap model to use for summarisation.
        context_ids: Message IDs already in the context window — these are
                     excluded from the digest so we don't repeat information.
        bot_jid: Normalised bot JID for labelling.

    Returns:
        A 2-3 sentence English digest, or empty string if there's nothing
        worth summarising.
    """
    group_jid = message.chat_jid

    # Check cache first (fast path, no lock)
    if group_jid in _digest_cache:
        return _digest_cache[group_jid]

    # Acquire per-group lock to prevent duplicate LLM calls
    async with _locks_lock:
        if group_jid not in _digest_locks:
            _digest_locks[group_jid] = asyncio.Lock()
        lock = _digest_locks[group_jid]

    async with lock:
        # Double-check cache after acquiring lock
        if group_jid in _digest_cache:
            return _digest_cache[group_jid]

        return await _generate_digest(session, message, model_name, context_ids, bot_jid)


async def _generate_digest(
    session: AsyncSession,
    message: Message,
    model_name: str,
    context_ids: Set[str],
    bot_jid: str | None,
) -> str:
    """Generate and cache a conversation digest (called under lock)."""
    group_jid = message.chat_jid

    cutoff = datetime.now(timezone.utc) - timedelta(hours=_LOOKBACK_HOURS)

    stmt = (
        select(Message)
        .where(Message.chat_jid == group_jid)
        .where(Message.timestamp >= cutoff)
        .order_by(desc(Message.timestamp))
        .limit(_MAX_DIGEST_MESSAGES)
    )
    res = await session.exec(stmt)
    candidates = list(res.all())

    # Filter out messages already in the context window
    digest_messages = [m for m in candidates if m.message_id not in context_ids]

    # Not enough substance to digest
    if len(digest_messages) < 5:
        _digest_cache[group_jid] = ""
        return ""

    # Build opt-out map for privacy
    all_jids = list({m.sender_jid for m in digest_messages})
    opt_out_map = await get_opt_out_map(session, all_jids)

    # Chronological order for the LLM
    sorted_messages = sorted(digest_messages, key=lambda m: m.timestamp)

    chat_text = chat2text(sorted_messages, opt_out_map, bot_jid)

    try:
        agent = Agent(model=model_name, system_prompt=_DIGEST_SYSTEM_PROMPT)
        result = await agent.run(
            f"Summarise this group conversation:\n\n{chat_text}"
        )
        digest = result.output.strip()
    except Exception:
        logger.exception("Failed to generate conversation digest for %s", group_jid)
        digest = ""

    _digest_cache[group_jid] = digest
    return digest
